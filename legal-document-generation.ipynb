{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11650984,"sourceType":"datasetVersion","datasetId":7311615}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:10:28.034805Z","iopub.execute_input":"2025-05-03T10:10:28.035057Z","iopub.status.idle":"2025-05-03T10:10:28.431436Z","shell.execute_reply.started":"2025-05-03T10:10:28.035032Z","shell.execute_reply":"2025-05-03T10:10:28.430600Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/philippine-law/philippine_laws.parquet\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T14:21:56.632494Z","iopub.execute_input":"2025-05-05T14:21:56.633386Z","iopub.status.idle":"2025-05-05T14:21:56.638118Z","shell.execute_reply.started":"2025-05-05T14:21:56.633347Z","shell.execute_reply":"2025-05-05T14:21:56.637337Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"! pip install --upgrade pip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T14:23:13.247647Z","iopub.execute_input":"2025-05-05T14:23:13.247962Z","iopub.status.idle":"2025-05-05T14:23:18.223023Z","shell.execute_reply.started":"2025-05-05T14:23:13.247935Z","shell.execute_reply":"2025-05-05T14:23:18.222316Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\nCollecting pip\n  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\nDownloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.1.2\n    Uninstalling pip-24.1.2:\n      Successfully uninstalled pip-24.1.2\nSuccessfully installed pip-25.1.1\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"! pip install chromadb -q\n! pip install -q \"google-genai==1.7.0\" ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T14:23:23.852224Z","iopub.execute_input":"2025-05-05T14:23:23.852992Z","iopub.status.idle":"2025-05-05T14:23:30.007068Z","shell.execute_reply.started":"2025-05-05T14:23:23.852955Z","shell.execute_reply":"2025-05-05T14:23:30.006222Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngoogle-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"! pip install --upgrade google-generativeai -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T14:23:30.090868Z","iopub.execute_input":"2025-05-05T14:23:30.091152Z","iopub.status.idle":"2025-05-05T14:23:33.045132Z","shell.execute_reply.started":"2025-05-05T14:23:30.091130Z","shell.execute_reply":"2025-05-05T14:23:33.044172Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nopentelemetry-proto 1.32.1 requires protobuf<6.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\ntensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngoogle-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Tuple\nfrom sentence_transformers import SentenceTransformer\nfrom chromadb import PersistentClient\nfrom chromadb.config import Settings\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.preprocessing import normalize\nfrom google import genai\nfrom google.generativeai import types\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:06:12.066261Z","iopub.execute_input":"2025-05-05T15:06:12.066976Z","iopub.status.idle":"2025-05-05T15:06:12.071087Z","shell.execute_reply.started":"2025-05-05T15:06:12.066945Z","shell.execute_reply":"2025-05-05T15:06:12.070303Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# =======================\n# 1. Configure Gemini API\n# =======================\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\nclient = genai.Client(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:06:12.299091Z","iopub.execute_input":"2025-05-05T15:06:12.299338Z","iopub.status.idle":"2025-05-05T15:06:12.565166Z","shell.execute_reply.started":"2025-05-05T15:06:12.299322Z","shell.execute_reply":"2025-05-05T15:06:12.564372Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# ================================\n# 2. Gemini Embedding Function\n# ================================\ndef embedding_function(texts: List[str]) -> List[List[float]]:\n    embeddings = client.models.embed_content(\n        model=\"gemini-embedding-exp-03-07\",\n        contents=texts,\n        config=types.EmbedContentConfig(task_type=\"SEMANTIC_SIMILARITY\"))\n    return embeddings[\"embedding\"] if isinstance(texts, str) else embeddings[\"embeddings\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:06:12.571317Z","iopub.execute_input":"2025-05-05T15:06:12.571531Z","iopub.status.idle":"2025-05-05T15:06:12.576006Z","shell.execute_reply.started":"2025-05-05T15:06:12.571515Z","shell.execute_reply":"2025-05-05T15:06:12.575286Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# ===========================================\n# 3. Load Data and Paragraph-wise Chunking\n# ===========================================\ndef load_and_chunk_data(parquet_path: str = \"/kaggle/input/philippine-law\", max_chars: int = 2000):\n    df = pd.read_parquet(parquet_path)\n    texts = df[\"text\"].tolist()\n\n    def paragraphwise_chunking(text: str) -> List[str]:\n        paragraphs = text.split(\"\\n\\n\")\n        chunks, current_chunk = [], \"\"\n        for para in paragraphs:\n            if len(current_chunk) + len(para) < max_chars:\n                current_chunk += para + \"\\n\\n\"\n            else:\n                if current_chunk:\n                    chunks.append(current_chunk.strip())\n                current_chunk = para + \"\\n\\n\"\n        if current_chunk:\n            chunks.append(current_chunk.strip())\n        return chunks\n\n    chunked_text = []\n    doc_idx = []\n\n    for i, text in enumerate(texts):\n        chunks = paragraphwise_chunking(text)\n        chunked_text.extend(chunks)\n        doc_idx.extend([i] * len(chunks))\n\n    metadatas_raw = df[[\"citation_information\", \"url\", \"label\"]].to_dict(orient=\"records\")\n    metadatas_final = []\n    for i in doc_idx:\n        meta = metadatas_raw[i].copy()\n        meta[\"doc_idx\"] = i\n        metadatas_final.append(meta)\n\n    return chunked_text, metadatas_final","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:06:12.710276Z","iopub.execute_input":"2025-05-05T15:06:12.710541Z","iopub.status.idle":"2025-05-05T15:06:12.717153Z","shell.execute_reply.started":"2025-05-05T15:06:12.710522Z","shell.execute_reply":"2025-05-05T15:06:12.716498Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# ================================\n# 4. Set Up and Populate ChromaDB\n# ================================\ndef batch_add(collection, documents, embeddings, metadatas, ids, batch_size=5000):\n    for i in range(0, len(documents), batch_size):\n        collection.add(\n            documents=documents[i:i + batch_size],\n            embeddings=embeddings[i:i + batch_size],\n            metadatas=metadatas[i:i + batch_size],\n            ids=ids[i:i + batch_size]\n        )\n\ndef setup_vector_db(chunked_text: List[str], metadatas: List[dict], path=\"./chroma_db\"):\n    local_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n    chroma_client = PersistentClient(path=path)\n    collection = chroma_client.get_or_create_collection(name=\"philippine_omnicorpus_local\")\n\n    local_embeddings = local_model.encode(chunked_text, show_progress_bar=True, convert_to_numpy=True)\n    ids = [f\"doc_{i}\" for i in range(len(chunked_text))]\n\n    batch_add(collection, chunked_text, local_embeddings.tolist(), metadatas, ids)\n    return collection, local_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:06:13.169303Z","iopub.execute_input":"2025-05-05T15:06:13.169834Z","iopub.status.idle":"2025-05-05T15:06:13.175586Z","shell.execute_reply.started":"2025-05-05T15:06:13.169811Z","shell.execute_reply":"2025-05-05T15:06:13.174803Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"# ============================\n# 5. Hybrid Search + Re-Rank\n# ============================\ndef hybrid_search(query: str, collection, local_model, top_k: int = 10):\n    q_emb = local_model.encode([query], convert_to_numpy=True).tolist()\n    results = collection.query(\n        query_embeddings=q_emb,\n        n_results=top_k,\n        include=[\"documents\", \"metadatas\"]\n    )\n    return results[\"documents\"][0], results[\"metadatas\"][0]\n\ndef rerank_chunks(query: str, candidate_chunks: List[str], candidate_metas: List[dict],\n                  local_model, fusion_weight: float = 0.5, top_k: int = 10):\n\n    local_query_emb = normalize(local_model.encode([query], convert_to_numpy=True))\n    local_chunk_embs = normalize(local_model.encode(candidate_chunks, convert_to_numpy=True))\n\n    gemini_query_emb = normalize(np.array(embedding_function(query)).reshape(1, -1))\n    gemini_chunk_embs = normalize(np.array(embedding_function(candidate_chunks)))\n\n    local_scores = cosine_similarity(local_query_emb, local_chunk_embs)[0]\n    gemini_scores = cosine_similarity(gemini_query_emb, gemini_chunk_embs)[0]\n\n    combined_scores = fusion_weight * local_scores + (1 - fusion_weight) * gemini_scores\n    sorted_indices = np.argsort(combined_scores)[::-1][:top_k]\n    return [(candidate_chunks[i], candidate_metas[i], combined_scores[i]) for i in sorted_indices]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:06:13.411809Z","iopub.execute_input":"2025-05-05T15:06:13.412111Z","iopub.status.idle":"2025-05-05T15:06:13.418619Z","shell.execute_reply.started":"2025-05-05T15:06:13.412089Z","shell.execute_reply":"2025-05-05T15:06:13.417939Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"# ===========================\n# 6. Generate Legal Response\n# ===========================\ndef generate_legal_draft(query: str, context_chunks: List[Tuple[str, dict, float]], top_k: int = 3):\n    top_context = \"\\n\\n\".join([chunk[0] for chunk in context_chunks[:top_k]])\n    top_metadata = [chunk[1] for chunk in context_chunks[:top_k]]\n\n    metadata_info = \"\\n\".join([\n        f\"Doc {meta['doc_idx']} | Citation: {meta['citation_information']} | URL: {meta['url']} | Label: {meta['label']}\"\n        for meta in top_metadata\n    ])\n\n    prompt = f\"\"\"You are a legal assistant. Based on the following legal context and metadata, \ndraft a precise and professional legal clause or document section.\n\nContext:\n{top_context}\n\nMetadata:\n{metadata_info}\n\nInstruction:\n{query}\n\"\"\"\n\n    response = genai.GenerativeModel(\"gemini-pro\").generate_content(prompt)\n    return response.text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:06:13.645039Z","iopub.execute_input":"2025-05-05T15:06:13.645223Z","iopub.status.idle":"2025-05-05T15:06:13.650244Z","shell.execute_reply.started":"2025-05-05T15:06:13.645209Z","shell.execute_reply":"2025-05-05T15:06:13.649556Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"# ========================\n# 7. Main Function\n# ========================\ndef main():\n    print(\"🔹 Loading and chunking data...\")\n    chunked_text, metadatas = load_and_chunk_data()\n\n    print(\"🔹 Setting up vector database...\")\n    collection, local_model = setup_vector_db(chunked_text, metadatas)\n\n    query = input(\"Enter your legal drafting instruction: \")\n    print(f\"🔹 Running hybrid search for query: {query}\")\n    candidate_chunks, candidate_metas = hybrid_search(query, collection, local_model)\n\n    print(\"🔹 Re-ranking top candidates using Gemini + Local Embeddings...\")\n    reranked = rerank_chunks(query, candidate_chunks, candidate_metas, local_model)\n\n    print(\"🔹 Generating legal draft...\")\n    legal_draft = generate_legal_draft(query, reranked)\n\n    print(\"\\n LEGAL DRAFT OUTPUT:\")\n    print(legal_draft)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:06:13.862644Z","iopub.execute_input":"2025-05-05T15:06:13.862842Z","iopub.status.idle":"2025-05-05T15:06:13.867543Z","shell.execute_reply.started":"2025-05-05T15:06:13.862827Z","shell.execute_reply":"2025-05-05T15:06:13.866844Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:06:14.179819Z","iopub.execute_input":"2025-05-05T15:06:14.180514Z","execution_failed":"2025-05-05T16:45:17.413Z"}},"outputs":[{"name":"stdout","text":"🔹 Loading and chunking data...\n🔹 Setting up vector database...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2853 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f230b3605824e0ab2d134c1032cb0f0"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}